import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from sklearn.utils import class_weight
import matplotlib.pyplot as plt
import numpy as np
import os
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score

# Enable memory growth for GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Dataset directories
train_dir = 'veri_seti/train'
test_dir = 'veri_seti/val'

# Function to resize images with padding to a target size (for 224x224)
def resize_with_padding(image, target_size=(224, 224)):
    height, width, _ = image.shape
    target_height, target_width = target_size

    # Maintain aspect ratio
    scale = min(target_width / width, target_height / height)
    new_width = int(width * scale)
    new_height = int(height * scale)

    # Resize the image
    image = tf.image.resize(image, (new_height, new_width))

    # Add padding
    padded_img = tf.image.pad_to_bounding_box(
        image,
        (target_height - new_height) // 2,
        (target_width - new_width) // 2,
        target_height,
        target_width
    )

    return padded_img

# Custom preprocessing function for ImageDataGenerator
def custom_preprocessing(image):
    image = resize_with_padding(image, target_size=(224, 224))
    return image

# Data augmentation for the training set
train_datagen = ImageDataGenerator(
    preprocessing_function=custom_preprocessing,
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    brightness_range=[0.85, 1.15],
    channel_shift_range=0.3,
    fill_mode='nearest'
)

# Preprocessing for the validation/test set (no augmentation)
test_datagen = ImageDataGenerator(
    preprocessing_function=custom_preprocessing
)

# Load training data from directory
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=16,  # Increased batch size
    class_mode='categorical',
    shuffle=True
)

# Load validation data from directory
validation_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,  # Increased batch size
    class_mode='categorical',
    shuffle=False
)

# Check for class imbalance and compute class weights
class_weights = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weights_dict = dict(enumerate(class_weights))

# Build the model (EfficientNet-B0)
base_model = tf.keras.applications.EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Fine-tuning: Unfreeze more layers
base_model.trainable = True
for layer in base_model.layers[:-20]:
    layer.trainable = False

# Add custom layers for classification
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005)),
    layers.BatchNormalization(),
    layers.Dropout(0.2),
    layers.Dense(train_generator.num_classes, activation='softmax')
])

# Optimizer (SGD with momentum)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9, nesterov=True)

# Compile the model
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define callbacks
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'best_model_weights.weights.h5',  # File path suitable for Colab
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    save_weights_only=True,
    verbose=1
)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=15,
    restore_best_weights=True,
    mode='max'
)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    min_lr=1e-6
)

# Train the model
history = model.fit(
    train_generator,
    epochs=100,
    validation_data=validation_generator,
    callbacks=[early_stopping, reduce_lr, checkpoint],
    class_weight=class_weights_dict
)

# Load the best weights
weights_path = 'best_model_weights.weights.h5'
if os.path.exists(weights_path):
    model.load_weights(weights_path)
else:
    print(f"Weight file not found: {weights_path}")

# Function to predict the class of a single image
def predict_image(image_path):
    img = load_img(image_path)
    img_array = img_to_array(img)
    img_array = resize_with_padding(img_array, target_size=(224, 224))
    img_array = np.expand_dims(img_array, axis=0)

    predictions = model.predict(img_array)
    predicted_class = list(train_generator.class_indices.keys())[np.argmax(predictions[0])]
    confidence = np.max(predictions[0]) * 100

    print(f"Predicted class: {predicted_class}\nConfidence: {confidence:.2f}%")

# Save the entire model (including architecture and weights)
# model.save("trained_model.h5") # Saving the full model might be too large for sharing weights is preferred

# Save only the model weights
model.save_weights("trained_model_weights.h5")

# Make predictions on the validation set
y_pred_probs = model.predict(validation_generator, verbose=1)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = validation_generator.classes

# Calculate evaluation metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
mcc = matthews_corrcoef(y_true, y_pred)

# Get class names
class_names = list(train_generator.class_indices.keys())

# Print evaluation metrics
print(f"\n--- Model Performance Metrics ---")
print(f"Accuracy    : {accuracy:.4f}")
print(f"Precision   : {precision:.4f}")
print(f"Recall      : {recall:.4f}")
print(f"F1 Score    : {f1:.4f}")
print(f"MCC         : {mcc:.4f}")

# Print classification report
print("\n--- Classification Report ---")
print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))

# Plot confusion matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate the model on the validation set
test_loss, test_accuracy = model.evaluate(validation_generator)
print(f"Validation Accuracy: {test_accuracy * 100:.2f}%")

# Visualize some misclassified examples
for true_idx in range(len(class_names)):
    for pred_idx in range(len(class_names)):
        if true_idx == pred_idx:
            continue

        misclassified_indices = [i for i in range(len(y_true)) if y_true[i] == true_idx and y_pred[i] == pred_idx]
        if not misclassified_indices:
            continue

        print(f"\nTrue: {class_names[true_idx]} → Predicted: {class_names[pred_idx]} ({len(misclassified_indices)} errors)")

        plt.figure(figsize=(15, 5))
        for i, idx in enumerate(misclassified_indices[:5]):
            img_path = validation_generator.filepaths[idx]
            img = load_img(img_path, target_size=(224, 224))
            plt.subplot(1, 5, i + 1)
            plt.imshow(img)
            plt.axis('off')
            plt.title(f"{class_names[true_idx]} → {class_names[pred_idx]}")
        plt.tight_layout()
        plt.show()

plt.tight_layout()
plt.show()
